---
title: "TP2"
author: "MARIAC Damien, DUIGOU Lucien"
date: "01/12/2026"
format:
  html:
    theme:
      light: [cosmo]
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    code-summary: "Afficher / masquer le code"
    code-copy: true
    highlight-style: github
    df-print: paged
    smooth-scroll: true
    anchor-sections: true
    fig-cap-location: bottom
    tbl-cap-location: top
execute:
  echo: true
  warning: false
  message: false
---

# Contexte

Il s'agit d'un autre jeu de données de lac (un différent).
Cette fois ci il y a des mesures de température de l'eau, de saturation en oxygène et de chlorophylle totale.
La température est mesurée à différents paliers de profondeur (palier 1 = surface, palier 5 = fond).
Les données couvrent la période de 2020 à mi 2022.
Il y a des trous dans les données, mais globalement la fréquence d'échantillonnage est bonne.
Votre tache sera de
- faire une analyse rapide de ces données
- Réduire le jeu de données à une fréquence de 1h15
- compléter les trous par interpolation raisonnable (pas "trop grand trou")
- traiter les outliers éventuels
- produire des visualisations claires et pertinentes

# Nettoyage des données

## Première approche : agrégation simple

Notre première approche naïve a été de simplement aggréger les données à une fréquence de 1h15 en utilisant la moyenne des observations dans chaque intervalle de temps.



Les données ne sont pas bien espacées dans le temps. Il y'a des données toutes les 20/30 secondes en moyenne mais aussi des trous de plusieurs heures.

```{r}
df <- read.csv("data_lake_2.csv")
df$DATE <- as.POSIXct(df$DATE, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
```

Voici une visualisation des écarts entre deux mesures successives. On remarque qu'il y'a des écarts très importants (plusieurs jours) mais aussi des écarts très faibles (quelques secondes).

```{r}
gaps <- as.numeric(diff(df$DATE))

gaps_extreme <- gaps[gaps>86400] # plus de 1j
hist(gaps_extreme,breaks=50)

gaps_moy <- gaps[gaps<=3600] # moins de 1h
hist(gaps_moy, breaks=50)
```



Notre stratégie pour nettoyer les données sera de fragmenter les données en blocs d'observations proches en date (moins de 100000 secondes d'écart entre deux observations successives qui est arbitraire par rapport à l'observations des histogramme au-dessus). On traitera ensuite chaque bloc indépendamment en agrégant les données à une fréquence de 1h15.


```{r}
# On fragmente les données en bloc d'observations proches en date
temps <- as.numeric(df$DATE)
n <- length(temps)
gaps <- diff(temps)

blocs <- list()
k <- 0
j <- 0
for(i in 1:(n-1)){
  if(gaps[i] > 100000){
    k <- k+1
    blocs[[k]] <- df[(j+1):i,]

    j <- i
  }else{}
}
k <- k + 1
blocs[[k]] <- df[(j+1):n, ]
blocs[[1]]

```






Puis on aggrège chaque bloc à une fréquence de 1h15 en utilisant la moyenne des observations dans chaque intervalle de temps.

```{r}
# fonction qui aggrège les données en blocs de 1h15

reducbloc <- function(X, reduc = 75*60){

  X$DATE <- as.POSIXct(X$DATE, format="%Y-%m-%d %H:%M:%S", tz="UTC")
  X <- X[order(X$DATE), ]

  X$Twater <- as.numeric(X$Twater)
  X$O2sat  <- as.numeric(X$O2sat)
  X$Chltot <- as.numeric(X$Chltot)

  temps <- as.numeric(X$DATE)
  n <- nrow(X)

  out <- data.frame(DATE=as.POSIXct(character()), Twater=numeric(),
                    O2sat=numeric(), Chltot=numeric(), stringsAsFactors=FALSE)

  j <- 1
  t0 <- temps[1]
  k <- 0

  for(i in 1:n){
    if(temps[i] - t0 >= reduc){
      k <- k + 1
      out[k, "DATE"]   <- X$DATE[j]
      out[k, "Twater"] <- mean(X$Twater[j:(i-1)], na.rm=TRUE)
      out[k, "O2sat"]  <- mean(X$O2sat [j:(i-1)], na.rm=TRUE)
      out[k, "Chltot"] <- mean(X$Chltot[j:(i-1)], na.rm=TRUE)

      j <- i
      t0 <- temps[i]
    }
  }

  # dernier bloc
  k <- k + 1
  out[k, "DATE"]   <- X$DATE[j]
  out[k, "Twater"] <- mean(X$Twater[j:n], na.rm=TRUE)
  out[k, "O2sat"]  <- mean(X$O2sat [j:n], na.rm=TRUE)
  out[k, "Chltot"] <- mean(X$Chltot[j:n], na.rm=TRUE)

  out
}
```





Puis on applique cette fonction à chaque bloc et on reconstitue le jeu de données nettoyé. Et on concatène les blocs nettoyés pour reconstituer le jeu de données nettoyé.


```{r}
agg_list <- list()

for(b in 1:length(blocs)){
  agg_list[[b]] <- reducbloc(blocs[[b]], reduc = 75*60)
}

df75 <- do.call(rbind, agg_list)

df75 <- df75[order(df75$DATE), ]

head(df75)
nrow(df75)
```

On plot les données nettoyées pour Twater.

```{r}
plot(df$DATE, df$Twater, type="l")
lines(df75$DATE, df75$Twater, type="l",col="red")
```

Cette approche est simple à mettre en place mais elle présente des inconvénients. En effet, on moyenne les données sur des paliers de profondeur peuvent être différents. Or, certaines variables peuvent varier en fonction de la profondeur (notamment la température de l'eau). On va donc essayer une autre approche qui consiste à aggréger les données en fonction du palier de profondeur.


## Deuxième approche : Agrégation en fonction du palier de profondeur

Il est possible que certaines variables varie en fonction de la profondeur. Nous allons donc essayer une autre approche qui consiste à aggréger les données en fonction du palier de profondeur. Cela permettra de mieux capturer les variations de chaque variable en fonction de la profondeur plutot que de moyenner entre paliers différents.

C'est cette approche qu'on gardera pour la suite de l'analyse.

```{r}
df <- read.csv("data_lake_2.csv")
df$DATE <- as.POSIXct(df$DATE, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")

P1 <- df[df$palier == 1,]
P2 <- df[df$palier == 2,]
P3 <- df[df$palier == 3,]
P4 <- df[df$palier == 4,]
P5 <- df[df$palier == 5,]
```


Pour chaque palier, on fragmente les données en blocs d'observations proches en date (moins de 100000 secondes d'écart entre deux observations successives qui est arbitraire par rapport à l'observations des histogramme au-dessus). On traitera ensuite chaque bloc indépendamment en agrégant les données à une fréquence de 1h15.

Cette fois on crée une fonction pour pouvoir réutiliser le code plus facilement (5 fois pour les 5 paliers).

```{r}
# On fragmente les données en bloc d'observations proches en date pour le palier 1

frag <- function(X, gap = 100000){
  X <- X[order(X$DATE),]
  t <- as.numeric(X$DATE)
  n <- length(t)
  if(n <= 1) return(list(X)) # on sait jamais

  gaps <- diff(t)

  blocs <- list()
  k <- 0
  j <- 0

  for(i in 1:(n-1)){
    if(gaps[i] > gap){
      k <- k + 1
      blocs[[k]] <- X[(j+1):i, , drop = FALSE]
      j <- i
    }
  }

  # dernier bloc
  k <- k + 1
  blocs[[k]] <- X[(j+1):n, , drop = FALSE]

  blocs
}

blocs1 <- frag(P1, gap = 100000)
blocs2 <- frag(P2, gap = 100000)
blocs3 <- frag(P3, gap = 100000)
blocs4 <- frag(P4, gap = 100000)
blocs5 <- frag(P5, gap = 100000)




plot(blocs1[[1]]$DATE, blocs1[[1]]$Twater, type="l",main = " Exemple d'un Bloc ( bloc 1 de palier 1")
plot(blocs1[[2]]$DATE, blocs1[[2]]$Twater, type="l",main = " Exemple d'un Bloc ( bloc 2 de palier 1")
```


On crée une fonction d'agrégation pour un bloc de données. On calcule la moyenne des observations dans chaque intervalle de temps de 1h15 au maximum.

Il peut y avoir des observations trop espacées (> 1h15). On décide donc de moyenner les observations dans des fenêtres de temps de 1h15 maximum. Si les observations sont trop espacées, on crée plusieurs moyennes dans le bloc.

```{r}

aggregv2 <- function(X, temps = 75*60){
  X <- X[order(X$DATE), ]
  n <- nrow(X)

  out <- data.frame(DATE = as.POSIXct(character()),Twater = numeric(),O2sat  = numeric(),
    Chltot = numeric())

  i <- 1
  while(i <= n){
    j <- i
    # avancer j tant que on reste dans la fenêtre et dans les bornes
    while(j < n && as.numeric(difftime(X$DATE[j+1], X$DATE[i], units="secs")) < temps){
      j <- j + 1
    }

    out <- rbind(out, data.frame(
      DATE   = X$DATE[i],
      Twater = mean(X$Twater[i:j], na.rm = TRUE),
      O2sat  = mean(X$O2sat[i:j],  na.rm = TRUE),
      Chltot = mean(X$Chltot[i:j], na.rm = TRUE)
    ))

    i <- j + 1
  }

  out
}
```



Ici, on applique la fonction d'agrégation à chaque fragment de chaque palier, puis on reconstitue le jeu de données agrégé pour chaque palier.


```{r}
# On applique la fonction à chaque fragment du jeu de données

aggregtout <- function(X, temps = 75*60){
  out_list <- lapply(X, aggregv2, temps = temps)
  out <- do.call(rbind, out_list)
  rownames(out) <- NULL
  out
}

aggregtout_P1 <- aggregtout(blocs1, temps = 75*60)
aggregtout_P2 <- aggregtout(blocs2, temps = 75*60)
aggregtout_P3 <- aggregtout(blocs3, temps = 75*60)
aggregtout_P4 <- aggregtout(blocs4, temps = 75*60)
aggregtout_P5 <- aggregtout(blocs5, temps = 75*60)
```


Voici une visualisation des données agrégées pour chaque palier, en comparant les données originales et les données agrégées.

```{r,fig.width=20, fig.height=16}
op <- par(mfrow=c(5,1), mar=c(3,4,2,1), oma=c(2,5,2,1))

plot(P1$DATE, P1$Twater, cex=0.2, xlab="", ylab="Twater au palier 1", main="P1",col="blue")
points(aggregtout_P1$DATE, aggregtout_P1$Twater, col="red", cex=0.2)

plot(P2$DATE, P2$Twater, cex=0.2, xlab="", ylab="Twater au palier 2", main="P2",col="blue")
points(aggregtout_P2$DATE, aggregtout_P2$Twater, col="red", cex=0.2)

plot(P3$DATE, P3$Twater, cex=0.2, xlab="", ylab="Twater au palier 3", main="P3",col="blue")
points(aggregtout_P3$DATE, aggregtout_P3$Twater, col="red", cex=0.2)

plot(P4$DATE, P4$Twater, cex=0.2, xlab="", ylab="Twater au palier 4", main="P4",col="blue")
points(aggregtout_P4$DATE, aggregtout_P4$Twater, col="red", cex=0.2)

plot(P5$DATE, P5$Twater, cex=0.2, xlab="Date", ylab="Twater au palier 5", main="P5",col="blue")
points(aggregtout_P5$DATE, aggregtout_P5$Twater, col="red", cex=0.2)

mtext("DATE", side=1, outer=TRUE, line=0.5)
mtext("Température eau (Twater)", side=2, outer=TRUE, line=2.5)

par(op)
```

On voit que les données agrégées semblent superposer les données originales à quelques exceptions près (notamment pour le palier 5 entre 2021 et 2022).



On plot la différence absolue entre les données originales et les données agrégées pour le palier 1.

```{r}


idx <- match(aggregtout_P1$DATE, P1$DATE)
d <- P1$Twater[idx] - aggregtout_P1$Twater

summary(d)
summary(abs(d))

plot(aggregtout_P1$DATE, abs(d), type="p", cex=0.4,
     xlab="Date", ylab="|Brut - Moyenne(75min)|")
abline(h=c(0.01, 0.05, 0.1), lty=2)


d <- P1$O2sat[idx] - aggregtout_P1$O2sat

plot(aggregtout_P1$DATE, abs(d), type="p", cex=0.4,
     xlab="Date", ylab="|Brut - Moyenne(75min)|")
abline(h=c(0.01, 0.05, 0.1), lty=2)


d <- P1$Chltot[idx] - aggregtout_P1$Chltot

plot(aggregtout_P1$DATE, abs(d), type="p", cex=0.4,
     xlab="Date", ylab="|Brut - Moyenne(75min)|")
abline(h=c(0.01, 0.05, 0.1), lty=2)

```

Les données agrégées semblent être une bonne approximation des données originales. On peut donc utiliser ces données agrégées pour la suite de l'analyse. On divise par 3 le nombre de points de données (de 18000 à 6000 pour un palier).

Comme les écarts entre les données initiales et les données aggrégées sembles raisonnablement faibles, on admet l'hypothèse qu'il n'y a pas d'outliers dans nos jeux de données.

## Traitement des données maquantes

Il reste maintenant à traiter les données manquantes (NA) dans nos jeux de données nettoyés. Nous allons utiliser un algorithme EM pour estimer les valeurs manquantes en supposant que les données suivent une distribution normale multivariée.

Pour cela, nous allons implémenter l'algorithme EM que nous avons codé précédemment dans le TP3.

```{r}
EM <- function(X, max_iter=50, tol=1e-4, ridge=1e-6) {
  X <- as.matrix(X)
  n <- nrow(X)
  p <- ncol(X)
  miss <- is.na(X)

  #INITIALISATION
  Ximp <- X
  cm <- colMeans(X, na.rm=TRUE)
  for (j in 1:p){
    Ximp[miss[,j], j] <- cm[j]
  }

  mu <- colMeans(Ximp)
  S <- cov(Ximp) + ridge*diag(p)

  for (it in 1:max_iter) {
    mu0 <- mu; S0 <- S



    #ETAPE E
    for (i in 1:n) {
      mi <- miss[i, ]
      if (!any(mi)) next
      oi <- !mi

      Soo <- S[oi, oi, drop=FALSE] + ridge*diag(sum(oi))
      Sm_o <- S[mi, oi, drop=FALSE]

      x_o <- Ximp[i, oi]
      Ximp[i, mi] <- as.numeric(mu[mi] + Sm_o %*% solve(Soo) %*% (x_o - mu[oi]))
    }


    #ETAPE M
    mu <- colMeans(Ximp)
    S  <- cov(Ximp) + ridge*diag(p)

    dmu <- norm(mu - mu0, "2") / (norm(mu0, "2") + 1e-12)
    dS  <- norm(S - S0,  "F") / (norm(S0,  "F") + 1e-12)
    if (max(dmu, dS) < tol) break
  }

  list(X_imp = Ximp, mu = mu, Sigma = S, n_iter = it)
}
```

On implémente l'algorithme EM sur chaque palier de profondeur puis on ajoute la colonne DATE pour pouvoir visualiser les résultats.

```{r}
P1EM <- as.data.frame(EM(aggregtout_P1[,2:4])$X_imp)
P1EM$DATE <- aggregtout_P1$DATE

P2EM <-  as.data.frame(EM(aggregtout_P2[,2:4])$X_imp)
P2EM$DATE <- aggregtout_P2$DATE

P3EM <-  as.data.frame(EM(aggregtout_P3[,2:4])$X_imp)
P3EM$DATE <- aggregtout_P3$DATE

P4EM <-  as.data.frame(EM(aggregtout_P4[,2:4])$X_imp)
P4EM$DATE <- aggregtout_P4$DATE

P5EM <-  as.data.frame(EM(aggregtout_P5[,2:4])$X_imp)
P5EM$DATE <- aggregtout_P5$DATE
```

On trace ici la superposition des données de saturation en O2 et les données de Chlorophyle totale pour chaque palier. 
```{r}
plot(aggregtout_P1$DATE, aggregtout_P1$O2sat, type="p", pch=16, cex=0.2)
points(P1EM$DATE, P1EM$O2sat , type="p", pch=16, cex=0.2)
points(P1EM$DATE, P1EM$Chltot , type="p", pch=16, cex=0.2,col="blue")

plot(aggregtout_P2$DATE, aggregtout_P2$O2sat, type="p", pch=16, cex=0.2)
points(P2EM$DATE, P2EM$O2sat , type="p", pch=16, cex=0.2)
points(P2EM$DATE, P2EM$Chltot , type="p", pch=16, cex=0.2,col="blue")

plot(aggregtout_P3$DATE, aggregtout_P3$O2sat, type="p", pch=16, cex=0.2)
points(P3EM$DATE, P3EM$O2sat , type="p", pch=16, cex=0.2)
points(P3EM$DATE, P3EM$Chltot , type="p", pch=16, cex=0.2,col="blue")

plot(aggregtout_P4$DATE, aggregtout_P4$O2sat, type="p", pch=16, cex=0.2)
points(P4EM$DATE, P4EM$O2sat , type="p", pch=16, cex=0.2)
points(P4EM$DATE, P4EM$Chltot , type="p", pch=16, cex=0.2,col="blue")

plot(aggregtout_P5$DATE, aggregtout_P5$O2sat, type="p", pch=16, cex=0.2)
points(P5EM$DATE, P5EM$O2sat , type="p", pch=16, cex=0.2)
points(P5EM$DATE, P5EM$Chltot , type="p", pch=16, cex=0.2,col="blue")
```

On remarque que lorsque que la concentration en Chlorophyle subit une baisse ou une hausse, la saturation en O2 suit la même tendance à un petit décalage près. Cela peut s'expliquer par le fait que la chlorophyle est responsable de la croissance des végétaux ainsi que de leur processus de production en 02. C'est pourquoi une augmentation de la concentration en Chlorophyle s'accompagne d'une augmentation de saturation en O2. 
Nous remarquons également que plus nous mesurons en profondeur, plus les données sont faibles (la différence entre le palier 1 et 5 est fine mais observable cela se remarque essentiellement lors de gros pics de données comme durant l'été 2021 ou on remarque une différence d'environ 50% pour la saturation en O2 entre ces deux paliers). En effet, la production d'O2 chez les végétaux se fait grâce à la Chlorophyle par mécanisme de photosynthèse. Or, plus nous sommes en profondeur, moins nous subissons d'ensoleillement. Ainsi la production d'Oxygène est moins importante en profondeur qu'en surface. 

# Visualisation des données

## Boîte à moustaches

On concatène les données nettoyées pour chaque palier dans un seul dataframe pour faciliter la visualisation. On se penche sur l'idée de faire des boîtes à moustaches pour chaque palier et chaque variable.

```{r}
library(dplyr)

P1EM$palier <- 1
P2EM$palier <- 2
P3EM$palier <- 3
P4EM$palier <- 4
P5EM$palier <- 5

data <- bind_rows(P1EM, P2EM, P3EM, P4EM, P5EM) %>%
  mutate(palier = factor(palier))
```

```{r}
library(ggplot2)


ggplot(data, aes(x = palier, y = Twater, fill = palier)) +
  geom_boxplot() +
  labs(x="Palier", y="Twater") +
  theme_minimal()

ggplot(data, aes(x = palier, y = Twater, fill = palier)) +
  geom_boxplot() +
  labs(x="Palier", y="O2sat") +
  theme_minimal()

ggplot(data, aes(x = palier, y = Twater, fill = palier)) +
  geom_boxplot() +
  labs(x="Palier", y="Chltot") +
  theme_minimal()

```


Il n'y a pas de différences visibles entre les palier pour chaque variable. Bien que les variables on l'air de plus se concentrer autour de la moyenne pour de paliers plus grand.

Ce dont on aurait du s'attendre d'après les premièrs graphiques. En effet, on capte à quelques exceptions près les mêmes tendances pour chaque palier.

## Étude de la volatilité en fonction du palier

En revanche, quelque chose d'intéressant à observer est la volatilité de chaque variable en fonction du palier. En effet, pour la palier 1, il semble que la courbe semble beacoup plus érratique par rapport au palier 5 où la courbe et plus lisse. Nous allons donc étudier la volatilité de chaque variable en fonction du palier.


Notre idée et donc de calculer la variation absolue entre deux instants successifs pour chaque variable et chaque palier. C'est à dire :
$$
\Delta X_t = |X_t - X_{t-1}|
$$

Avec `X ` la variable considérée (`Twater, O2sat ou Chltot`) et `t` l'instant considéré.

```{r}
library(tidyr)

vol <- data %>%
  arrange(palier, DATE) %>%
  group_by(palier) %>%
  mutate(dTwater = abs(Twater - lag(Twater)),
         dO2sat  = abs(O2sat  - lag(O2sat)),
         dChltot = abs(Chltot - lag(Chltot))) %>%
  ungroup() %>%
  pivot_longer(cols = c(dTwater, dO2sat, dChltot),
               names_to = "variable",
               values_to = "abs_delta") %>%
  filter(is.finite(abs_delta)) %>%
  mutate(variable = recode(variable,
                           dTwater="Twater",
                           dO2sat="O2sat",
                           dChltot="Chltot"))

ggplot(vol, aes(x = palier, y = abs_delta, fill = palier)) +
  geom_boxplot(outlier.alpha = 0.2) +
  facet_wrap(~variable, scales = "free_y") +
  theme_minimal() +
  guides(fill = "none") +
  labs(x="Palier", y="|Δ| (variation entre 2 instants)")


```


La volatilité instantanée décroît globalement avec le palier pour `Twater` (série plus lisse aux paliers élevés).

On peut supposer que cela est du au fait que la température de l'eau est plus stable en profondeur qu'à la surface où elle est plus sujette aux variations climatiques (ensoleillement, vent, etc.).

Pour `O2sat` et `Chltot`, la volatilité typique varie peu selon le palier, mais on observe dans tous les cas des épisodes ponctuels de variations extrêmes (surement du à des outliers ou a des phénomènes très particulier qu'on ne pourrait expliquer).
