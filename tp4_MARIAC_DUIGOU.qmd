---
title: "TP2"
author: "MARIAC Damien, DUIGOU Lucien"
date: "01/12/2026"
format:
  html:
    theme:
      light: [cosmo]
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    code-summary: "Afficher / masquer le code"
    code-copy: true
    highlight-style: github
    df-print: paged
    smooth-scroll: true
    anchor-sections: true
    fig-cap-location: bottom
    tbl-cap-location: top
execute:
  echo: true
  warning: false
  message: false
---

# Contexte

Il s'agit d'un autre jeu de données de lac (un différent).
Cette fois ci il y a des mesures de température de l'eau, de saturation en oxygène et de chlorophylle totale.
La température est mesurée à différents paliers de profondeur (palier 1 = surface, palier 5 = fond).
Les données couvrent la période de 2020 à mi 2022.
Il y a des trous dans les données, mais globalement la fréquence d'échantillonnage est bonne.
Votre tache sera de
- faire une analyse rapide de ces données
- Réduire le jeu de données à une fréquence de 1h15
- compléter les trous par interpolation raisonnable (pas "trop grand trou")
- traiter les outliers éventuels
- produire des visualisations claires et pertinentes

# Nettoyage des données

## Première approche : agrégation simple

Les données ne sont pas bien espacées dans le temps. Il y'a des données toutes les 20/30 secondes en moyenne mais aussi des trous de plusieurs heures.

```{r}
df <- read.csv("data_lake_2.csv")
df$DATE <- as.POSIXct(df$DATE, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
```

Voici une visualisation des écarts entre deux mesures successives. On remarque qu'il y'a des écarts très importants (plusieurs jours) mais aussi des écarts très faibles (quelques secondes).

```{r}
gaps <- as.numeric(diff(df$DATE))

gaps_extreme <- gaps[gaps>86400] # plus de 1j
hist(gaps_extreme,breaks=50)

gaps_moy <- gaps[gaps<=3600] # moins de 1h
hist(gaps_moy, breaks=50)
```



Notre stratégie pour nettoyer les données sera de fragmenter les données en blocs d'observations proches en date (moins de 100000 secondes d'écart entre deux observations successives qui est arbitraire par rapport à l'observations des histogramme au-dessus). On traitera ensuite chaque bloc indépendamment en agrégant les données à une fréquence de 1h15.


```{r}
# On fragmente les données en bloc d'observations proches en date
temps <- as.numeric(df$DATE)
n <- length(temps)
gaps <- diff(temps)

blocs <- list()
k <- 0
j <- 0
for(i in 1:(n-1)){
  if(gaps[i] > 100000){
    k <- k+1
    blocs[[k]] <- df[(j+1):i,]

    j <- i
  }else{}
}
k <- k + 1
blocs[[k]] <- df[(j+1):n, ]
blocs[[1]]

```






Puis on aggrège chaque bloc à une fréquence de 1h15 en utilisant la moyenne des observations dans chaque intervalle de temps.

```{r}
# fonction qui aggrège les données en blocs de 1h15

reducbloc <- function(X, reduc = 75*60){

  X$DATE <- as.POSIXct(X$DATE, format="%Y-%m-%d %H:%M:%S", tz="UTC")
  X <- X[order(X$DATE), ]

  X$Twater <- as.numeric(X$Twater)
  X$O2sat  <- as.numeric(X$O2sat)
  X$Chltot <- as.numeric(X$Chltot)

  temps <- as.numeric(X$DATE)
  n <- nrow(X)

  out <- data.frame(DATE=as.POSIXct(character()), Twater=numeric(),
                    O2sat=numeric(), Chltot=numeric(), stringsAsFactors=FALSE)

  j <- 1
  t0 <- temps[1]
  k <- 0

  for(i in 1:n){
    if(temps[i] - t0 >= reduc){
      k <- k + 1
      out[k, "DATE"]   <- X$DATE[j]
      out[k, "Twater"] <- mean(X$Twater[j:(i-1)], na.rm=TRUE)
      out[k, "O2sat"]  <- mean(X$O2sat [j:(i-1)], na.rm=TRUE)
      out[k, "Chltot"] <- mean(X$Chltot[j:(i-1)], na.rm=TRUE)

      j <- i
      t0 <- temps[i]
    }
  }

  # dernier bloc
  k <- k + 1
  out[k, "DATE"]   <- X$DATE[j]
  out[k, "Twater"] <- mean(X$Twater[j:n], na.rm=TRUE)
  out[k, "O2sat"]  <- mean(X$O2sat [j:n], na.rm=TRUE)
  out[k, "Chltot"] <- mean(X$Chltot[j:n], na.rm=TRUE)

  out
}
```





Puis on applique cette fonction à chaque bloc et on reconstitue le jeu de données nettoyé. Et on concatène les blocs nettoyés pour reconstituer le jeu de données nettoyé.


```{r}
agg_list <- list()

for(b in 1:length(blocs)){
  agg_list[[b]] <- reducbloc(blocs[[b]], reduc = 75*60)
}

df75 <- do.call(rbind, agg_list)

df75 <- df75[order(df75$DATE), ]

head(df75)
nrow(df75)
```

On plot les données nettoyées pour Twater.

```{r}
plot(df$DATE, df$Twater, type="l")
lines(df75$DATE, df75$Twater, type="l",col="red")
```


## Deuxième approche : Agrégation en fonction du palier de profondeur

Il est possible que certaines variables varie en fonction de la profondeur. Nous allons donc essayer une autre approche qui consiste à aggréger les données en fonction du palier de profondeur. Cela permettra de mieux capturer les variations de chaque variable en fonction de la profondeur plutot que de moyenner entre paliers différents.

```{r}
df <- read.csv("data_lake_2.csv")
df$DATE <- as.POSIXct(df$DATE, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")

P1 <- df[df$palier == 1,]
P2 <- df[df$palier == 2,]
P3 <- df[df$palier == 3,]
P4 <- df[df$palier == 4,]
P5 <- df[df$palier == 5,]

```


Pour chaque palier, on fragmente les données en blocs d'observations proches en date (moins de 100000 secondes d'écart entre deux observations successives qui est arbitraire par rapport à l'observations des histogramme au-dessus). On traitera ensuite chaque bloc indépendamment en agrégant les données à une fréquence de 1h15.

Cette fois on crée une fonction pour pouvoir réutiliser le code plus facilement (5 fois pour les 5 paliers).

```{r}
# On fragmente les données en bloc d'observations proches en date pour le palier 1

frag <- function(X, gap = 100000){
  X <- X[order(X$DATE),]
  t <- as.numeric(X$DATE)
  n <- length(t)
  if(n <= 1) return(list(X)) # on sait jamais

  gaps <- diff(t)

  blocs <- list()
  k <- 0
  j <- 0

  for(i in 1:(n-1)){
    if(gaps[i] > gap){
      k <- k + 1
      blocs[[k]] <- X[(j+1):i, , drop = FALSE]
      j <- i
    }
  }

  # dernier bloc
  k <- k + 1
  blocs[[k]] <- X[(j+1):n, , drop = FALSE]

  blocs
}

blocs1 <- frag(P1, gap = 100000)
blocs2 <- frag(P2, gap = 100000)
blocs3 <- frag(P3, gap = 100000)
blocs4 <- frag(P4, gap = 100000)
blocs5 <- frag(P5, gap = 100000)


plot(blocs1[[1]]$DATE, blocs1[[1]]$Twater, type="l", main="Bloc 1 palier 1")
plot(blocs1[[2]]$DATE, blocs1[[2]]$Twater, type="l",main="Bloc 2 palier 1")

```


On crée une fonction d'agrégation pour un bloc de données. On calcule la moyenne des observations dans chaque intervalle de temps de 1h15 au maximum.

Il peut y avoir des observations trop espacées (> 1h15). On décide donc de moyenner les observations dans des fenêtres de temps de 1h15 maximum. Si les observations sont trop espacées, on crée plusieurs moyennes dans le bloc.

```{r}

aggregv2 <- function(X, temps = 75*60){
  X <- X[order(X$DATE), ]
  n <- nrow(X)

  out <- data.frame(DATE = as.POSIXct(character()),Twater = numeric(),O2sat  = numeric(),
    Chltot = numeric())

  i <- 1
  while(i <= n){
    j <- i
    # avancer j tant que on reste dans la fenêtre et dans les bornes
    while(j < n && as.numeric(difftime(X$DATE[j+1], X$DATE[i], units="secs")) < temps){
      j <- j + 1
    }

    out <- rbind(out, data.frame(
      DATE   = X$DATE[i],
      Twater = mean(X$Twater[i:j], na.rm = TRUE),
      O2sat  = mean(X$O2sat[i:j],  na.rm = TRUE),
      Chltot = mean(X$Chltot[i:j], na.rm = TRUE)
    ))

    i <- j + 1
  }

  out
}

aggregv2_bloc1 <- aggregv2(blocs1[[1]], temps = 75*60)
```



Ici, on applique la fonction d'agrégation à chaque fragment de chaque palier, puis on reconstitue le jeu de données agrégé pour chaque palier.


```{r}
# On applique la fonction à chaque fragment du jeu de données

aggregtout <- function(X, temps = 75*60){
  out_list <- lapply(X, aggregv2, temps = temps)
  out <- do.call(rbind, out_list)
  rownames(out) <- NULL
  out
}

aggregtout_P1 <- aggregtout(blocs1, temps = 75*60)
aggregtout_P2 <- aggregtout(blocs2, temps = 75*60)
aggregtout_P3 <- aggregtout(blocs3, temps = 75*60)
aggregtout_P4 <- aggregtout(blocs4, temps = 75*60)
aggregtout_P5 <- aggregtout(blocs5, temps = 75*60)
```


Voici une visualisation des données agrégées pour chaque palier, en comparant les données originales et les données agrégées.

```{r}
plot(P1$DATE, P1$Twater, type="l")
lines(aggregtout_P1$DATE, aggregtout_P1$Twater, col="red", type="l")

plot(P2$DATE, P2$Twater, type="l")
lines(aggregtout_P2$DATE, aggregtout_P2$Twater, col="red", type="l")

plot(P3$DATE, P3$Twater, type="l")
lines(aggregtout_P3$DATE, aggregtout_P3$Twater, col="red", type="l")

plot(P4$DATE, P4$Twater, type="l")
lines(aggregtout_P4$DATE, aggregtout_P4$Twater, col="red", type="l")

plot(P5$DATE, P5$Twater, type="l")
lines(aggregtout_P5$DATE, aggregtout_P5$Twater, col="red", type="l")

```

On que les données agrégées semblent superposer les données originales à quelques exceptions près (notamment pour le palier 5 entre 2021 et 2022).



On plot la différence absolue entre les données originales et les données agrégées pour le palier 1.

```{r}


idx <- match(aggregtout_P1$DATE, P1$DATE)
d <- P1$Twater[idx] - aggregtout_P1$Twater

summary(d)
summary(abs(d))

plot(aggregtout_P1$DATE, abs(d), type="p", cex=0.4,
     xlab="Date", ylab="|Brut - Moyenne(75min)|")
abline(h=c(0.01, 0.05, 0.1), lty=2)

```

Les données agrégées semblent être une bonne approximation des données originales. On peut donc utiliser ces données agrégées pour la suite de l'analyse. On divise par 3 le nombre de points de données (de 18000 à 6000 pour un palier).


# Traitement des outliers et des données manquantes

## Estimation de la densité par mélange gaussien

Algorithme EM
```{r}
EM=function(data, K=3, max_iter=100, tol=1e-6){
  ts=data[!is.na(data)]
  n=length(ts)

  mu_k=as.numeric(quantile(ts,probs=seq(0.25, 0.75, length.out=K)))
  sigma_k=rep(sd(ts), K)
  eps_k=rep(1/K, K)

  loglik_old=-Inf

  for (iter in 1:max_iter){

    # Etape E
    dens=sapply(1:K, function(k)
      eps_k[k]*dnorm(ts, mu_k[k], sigma_k[k])
    )

    dens_sum=rowSums(dens)
    dens_sum[dens_sum<tol]=tol

    gamma=dens/dens_sum

    # Etape M
    N_k=colSums(gamma)

    eps_k=N_k/n
    mu_k=colSums(gamma*matrix(ts,n,K))/N_k
    sigma_k=sqrt(colSums(gamma*sweep(matrix(ts,n,K), 2, mu_k, FUN="-")^2)/N_k)

    sigma_k[sigma_k<tol]=tol

    loglik=sum(log(dens_sum))

    if (abs(loglik - loglik_old)<tol) break
    loglik_old=loglik
  }

  list(eps=eps_k, mu=mu_k, sigma=sigma_k, loglik=loglik)
}
```

Estimation des paramètres pour chaque variable
```{r}
parametres=list(EM(df75$Twater),EM(df75$O2sat),EM(df75$Chltot))
```

Estimation de la densité pour chaque variable
```{r}
dens=function(x,param){
  d=c()
  l=length(x)
  for (i in 1:l){
    s=c()
    for (j in 1:3){
      s=c(s,param$eps[j]*dnorm(x[i],param$mu[j],param$sigma[j]))
    }
    d=c(d,sum(s))
  }
  return(d)
}

d=list(dens(df75$Twater,parametres[[1]]),dens(df75$O2sat,parametres[[2]]),dens(df75$Chltot,parametres[[3]]))
```

Attribution des NA pour des densité faibles
```{r}
clean=function(x,density,seuil=0.01){
  d=density
  l=length(x)
  clean=rep(NA,l)
  for (i in 1:l){
    if (d[i]>= seuil){
      clean[i]=x[i]
    }
  }
  return(clean)
}

test=clean(x=df75$Twater,density=d[[1]])
plot(df$DATE, df$Twater, type='o')
lines(df75$DATE, test, type='o',col='red')
```