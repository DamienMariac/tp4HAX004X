---
title: "TP2"
author: "MARIAC Damien, DUIGOU Lucien"
date: "01/12/2026"
format:
  html:
    theme:
      light: [cosmo]
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    code-summary: "Afficher / masquer le code"
    code-copy: true
    highlight-style: github
    df-print: paged
    smooth-scroll: true
    anchor-sections: true
    fig-cap-location: bottom
    tbl-cap-location: top
execute:
  echo: true
  warning: false
  message: false
---

# Contexte

Il s'agit d'un autre jeu de données de lac (un différent).
Cette fois ci il y a des mesures de température de l'eau, de saturation en oxygène et de chlorophylle totale.
La température est mesurée à différents paliers de profondeur (palier 1 = surface, palier 5 = fond).
Les données couvrent la période de 2020 à mi 2022.
Il y a des trous dans les données, mais globalement la fréquence d'échantillonnage est bonne.
Votre tache sera de
- faire une analyse rapide de ces données
- Réduire le jeu de données à une fréquence de 1h15
- compléter les trous par interpolation raisonnable (pas "trop grand trou")
- traiter les outliers éventuels
- produire des visualisations claires et pertinentes

# Nettoyage des données

## Première approche : agrégation simple

Notre première approche naïve a été de simplement aggréger les données à une fréquence de 1h15 en utilisant la moyenne des observations dans chaque intervalle de temps.



Les données ne sont pas bien espacées dans le temps. Il y'a des données toutes les 20/30 secondes en moyenne mais aussi des trous de plusieurs heures.

```{r}
df <- read.csv("data_lake_2.csv")
df$DATE <- as.POSIXct(df$DATE, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
```

Voici une visualisation des écarts entre deux mesures successives. On remarque qu'il y'a des écarts très importants (plusieurs jours) mais aussi des écarts très faibles (quelques secondes).

```{r}
gaps <- as.numeric(diff(df$DATE))

gaps_extreme <- gaps[gaps>86400] # plus de 1j
hist(gaps_extreme,breaks=50)

gaps_moy <- gaps[gaps<=3600] # moins de 1h
hist(gaps_moy, breaks=50)
```



Notre stratégie pour nettoyer les données sera de fragmenter les données en blocs d'observations proches en date (moins de 100000 secondes d'écart entre deux observations successives qui est arbitraire par rapport à l'observations des histogramme au-dessus). On traitera ensuite chaque bloc indépendamment en agrégant les données à une fréquence de 1h15.


```{r}
# On fragmente les données en bloc d'observations proches en date
temps <- as.numeric(df$DATE)
n <- length(temps)
gaps <- diff(temps)

blocs <- list()
k <- 0
j <- 0
for(i in 1:(n-1)){
  if(gaps[i] > 100000){
    k <- k+1
    blocs[[k]] <- df[(j+1):i,]

    j <- i
  }else{}
}
k <- k + 1
blocs[[k]] <- df[(j+1):n, ]
blocs[[1]]

```






Puis on aggrège chaque bloc à une fréquence de 1h15 en utilisant la moyenne des observations dans chaque intervalle de temps.

```{r}
# fonction qui aggrège les données en blocs de 1h15

reducbloc <- function(X, reduc = 75*60){

  X$DATE <- as.POSIXct(X$DATE, format="%Y-%m-%d %H:%M:%S", tz="UTC")
  X <- X[order(X$DATE), ]

  X$Twater <- as.numeric(X$Twater)
  X$O2sat  <- as.numeric(X$O2sat)
  X$Chltot <- as.numeric(X$Chltot)

  temps <- as.numeric(X$DATE)
  n <- nrow(X)

  out <- data.frame(DATE=as.POSIXct(character()), Twater=numeric(),
                    O2sat=numeric(), Chltot=numeric(), stringsAsFactors=FALSE)

  j <- 1
  t0 <- temps[1]
  k <- 0

  for(i in 1:n){
    if(temps[i] - t0 >= reduc){
      k <- k + 1
      out[k, "DATE"]   <- X$DATE[j]
      out[k, "Twater"] <- mean(X$Twater[j:(i-1)], na.rm=TRUE)
      out[k, "O2sat"]  <- mean(X$O2sat [j:(i-1)], na.rm=TRUE)
      out[k, "Chltot"] <- mean(X$Chltot[j:(i-1)], na.rm=TRUE)

      j <- i
      t0 <- temps[i]
    }
  }

  # dernier bloc
  k <- k + 1
  out[k, "DATE"]   <- X$DATE[j]
  out[k, "Twater"] <- mean(X$Twater[j:n], na.rm=TRUE)
  out[k, "O2sat"]  <- mean(X$O2sat [j:n], na.rm=TRUE)
  out[k, "Chltot"] <- mean(X$Chltot[j:n], na.rm=TRUE)

  out
}
```





Puis on applique cette fonction à chaque bloc et on reconstitue le jeu de données nettoyé. Et on concatène les blocs nettoyés pour reconstituer le jeu de données nettoyé.


```{r}
agg_list <- list()

for(b in 1:length(blocs)){
  agg_list[[b]] <- reducbloc(blocs[[b]], reduc = 75*60)
}

df75 <- do.call(rbind, agg_list)

df75 <- df75[order(df75$DATE), ]

head(df75)
nrow(df75)
```

On plot les données nettoyées pour Twater.

```{r}
plot(df$DATE, df$Twater, type="l")
lines(df75$DATE, df75$Twater, type="l",col="red")
```

Cette approche est simple à mettre en place mais elle présente des inconvénients. En effet, on moyenne les données sur des paliers de profondeur peuvent être différents. Or, certaines variables peuvent varier en fonction de la profondeur (notamment la température de l'eau). On va donc essayer une autre approche qui consiste à aggréger les données en fonction du palier de profondeur.


## Deuxième approche : Agrégation en fonction du palier de profondeur

Il est possible que certaines variables varie en fonction de la profondeur. Nous allons donc essayer une autre approche qui consiste à aggréger les données en fonction du palier de profondeur. Cela permettra de mieux capturer les variations de chaque variable en fonction de la profondeur plutot que de moyenner entre paliers différents.

C'est cette approche qu'on gardera pour la suite de l'analyse.

```{r}
df <- read.csv("data_lake_2.csv")
df$DATE <- as.POSIXct(df$DATE, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")

P1 <- df[df$palier == 1,]
P2 <- df[df$palier == 2,]
P3 <- df[df$palier == 3,]
P4 <- df[df$palier == 4,]
P5 <- df[df$palier == 5,]
```


Pour chaque palier, on fragmente les données en blocs d'observations proches en date (moins de 100000 secondes d'écart entre deux observations successives qui est arbitraire par rapport à l'observations des histogramme au-dessus). On traitera ensuite chaque bloc indépendamment en agrégant les données à une fréquence de 1h15.

Cette fois on crée une fonction pour pouvoir réutiliser le code plus facilement (5 fois pour les 5 paliers).

```{r}
# On fragmente les données en bloc d'observations proches en date pour le palier 1

frag <- function(X, gap = 100000){
  X <- X[order(X$DATE),]
  t <- as.numeric(X$DATE)
  n <- length(t)
  if(n <= 1) return(list(X)) # on sait jamais

  gaps <- diff(t)

  blocs <- list()
  k <- 0
  j <- 0

  for(i in 1:(n-1)){
    if(gaps[i] > gap){
      k <- k + 1
      blocs[[k]] <- X[(j+1):i, , drop = FALSE]
      j <- i
    }
  }

  # dernier bloc
  k <- k + 1
  blocs[[k]] <- X[(j+1):n, , drop = FALSE]

  blocs
}

blocs1 <- frag(P1, gap = 100000)
blocs2 <- frag(P2, gap = 100000)
blocs3 <- frag(P3, gap = 100000)
blocs4 <- frag(P4, gap = 100000)
blocs5 <- frag(P5, gap = 100000)




plot(blocs1[[1]]$DATE, blocs1[[1]]$Twater, type="l",main = " Exemple d'un Bloc ( bloc 1 de palier 1")
plot(blocs1[[2]]$DATE, blocs1[[2]]$Twater, type="l",main = " Exemple d'un Bloc ( bloc 2 de palier 1")
```


On crée une fonction d'agrégation pour un bloc de données. On calcule la moyenne des observations dans chaque intervalle de temps de 1h15 au maximum.

Il peut y avoir des observations trop espacées (> 1h15). On décide donc de moyenner les observations dans des fenêtres de temps de 1h15 maximum. Si les observations sont trop espacées, on crée plusieurs moyennes dans le bloc.

```{r}

aggregv2 <- function(X, temps = 75*60){
  X <- X[order(X$DATE), ]
  n <- nrow(X)

  out <- data.frame(DATE = as.POSIXct(character()),Twater = numeric(),O2sat  = numeric(),
    Chltot = numeric())

  i <- 1
  while(i <= n){
    j <- i
    # avancer j tant que on reste dans la fenêtre et dans les bornes
    while(j < n && as.numeric(difftime(X$DATE[j+1], X$DATE[i], units="secs")) < temps){
      j <- j + 1
    }

    out <- rbind(out, data.frame(
      DATE   = X$DATE[i],
      Twater = mean(X$Twater[i:j], na.rm = TRUE),
      O2sat  = mean(X$O2sat[i:j],  na.rm = TRUE),
      Chltot = mean(X$Chltot[i:j], na.rm = TRUE)
    ))

    i <- j + 1
  }

  out
}
```



Ici, on applique la fonction d'agrégation à chaque fragment de chaque palier, puis on reconstitue le jeu de données agrégé pour chaque palier.


```{r}
# On applique la fonction à chaque fragment du jeu de données

aggregtout <- function(X, temps = 75*60){
  out_list <- lapply(X, aggregv2, temps = temps)
  out <- do.call(rbind, out_list)
  rownames(out) <- NULL
  out
}

aggregtout_P1 <- aggregtout(blocs1, temps = 75*60)
aggregtout_P2 <- aggregtout(blocs2, temps = 75*60)
aggregtout_P3 <- aggregtout(blocs3, temps = 75*60)
aggregtout_P4 <- aggregtout(blocs4, temps = 75*60)
aggregtout_P5 <- aggregtout(blocs5, temps = 75*60)
```


Voici une visualisation des données agrégées pour chaque palier, en comparant les données originales et les données agrégées.

```{r,fig.width=20, fig.height=16}
op <- par(mfrow=c(5,1), mar=c(3,4,2,1), oma=c(2,5,2,1))

plot(P1$DATE, P1$Twater, cex=0.2, xlab="", ylab="Twater au palier 1", main="P1",col="blue")
points(aggregtout_P1$DATE, aggregtout_P1$Twater, col="red", cex=0.2)

plot(P2$DATE, P2$Twater, cex=0.2, xlab="", ylab="Twater au palier 2", main="P2",col="blue")
points(aggregtout_P2$DATE, aggregtout_P2$Twater, col="red", cex=0.2)

plot(P3$DATE, P3$Twater, cex=0.2, xlab="", ylab="Twater au palier 3", main="P3",col="blue")
points(aggregtout_P3$DATE, aggregtout_P3$Twater, col="red", cex=0.2)

plot(P4$DATE, P4$Twater, cex=0.2, xlab="", ylab="Twater au palier 4", main="P4",col="blue")
points(aggregtout_P4$DATE, aggregtout_P4$Twater, col="red", cex=0.2)

plot(P5$DATE, P5$Twater, cex=0.2, xlab="Date", ylab="Twater au palier 5", main="P5",col="blue")
points(aggregtout_P5$DATE, aggregtout_P5$Twater, col="red", cex=0.2)

mtext("DATE", side=1, outer=TRUE, line=0.5)
mtext("Température eau (Twater)", side=2, outer=TRUE, line=2.5)

par(op)
```

On voit que les données agrégées semblent superposer les données originales à quelques exceptions près (notamment pour le palier 5 entre 2021 et 2022).



On plot la différence absolue entre les données originales et les données agrégées pour le palier 1.

```{r}


idx <- match(aggregtout_P1$DATE, P1$DATE)
d <- P1$Twater[idx] - aggregtout_P1$Twater

summary(d)
summary(abs(d))

plot(aggregtout_P1$DATE, abs(d), type="p", cex=0.4,
     xlab="Date", ylab="|Brut - Moyenne(75min)|")
abline(h=c(0.01, 0.05, 0.1), lty=2)

```

Les données agrégées semblent être une bonne approximation des données originales. On peut donc utiliser ces données agrégées pour la suite de l'analyse. On divise par 3 le nombre de points de données (de 18000 à 6000 pour un palier).



## Traitement des données maquantes

Il reste maintenant à traiter les données manquantes (NA) dans nos jeux de données nettoyés. Nous allons utiliser un algorithme EM pour estimer les valeurs manquantes en supposant que les données suivent une distribution normale multivariée.

Pour cela, nous allons implémenter l'algorithme EM que nous avons codé précédemment dans le TP3.

```{r}
EM <- function(X, max_iter=50, tol=1e-4, ridge=1e-6) {
  X <- as.matrix(X)
  n <- nrow(X)
  p <- ncol(X)
  miss <- is.na(X)

  #INITIALISATION
  Ximp <- X
  cm <- colMeans(X, na.rm=TRUE)
  for (j in 1:p){
    Ximp[miss[,j], j] <- cm[j]
  }

  mu <- colMeans(Ximp)
  S <- cov(Ximp) + ridge*diag(p)

  for (it in 1:max_iter) {
    mu0 <- mu; S0 <- S



    #ETAPE E
    for (i in 1:n) {
      mi <- miss[i, ]
      if (!any(mi)) next
      oi <- !mi

      Soo <- S[oi, oi, drop=FALSE] + ridge*diag(sum(oi))
      Sm_o <- S[mi, oi, drop=FALSE]

      x_o <- Ximp[i, oi]
      Ximp[i, mi] <- as.numeric(mu[mi] + Sm_o %*% solve(Soo) %*% (x_o - mu[oi]))
    }


    #ETAPE M
    mu <- colMeans(Ximp)
    S  <- cov(Ximp) + ridge*diag(p)

    dmu <- norm(mu - mu0, "2") / (norm(mu0, "2") + 1e-12)
    dS  <- norm(S - S0,  "F") / (norm(S0,  "F") + 1e-12)
    if (max(dmu, dS) < tol) break
  }

  list(X_imp = Ximp, mu = mu, Sigma = S, n_iter = it)
}
```

On implémente l'algorithme EM sur chaque palier de profondeur puis on ajoute la colonne DATE pour pouvoir visualiser les résultats (pour Twater ici).

```{r}
P1EM <- as.data.frame(EM(aggregtout_P1[,2:4])$X_imp)
P1EM$DATE <- aggregtout_P1$DATE

P2EM <-  as.data.frame(EM(aggregtout_P2[,2:4])$X_imp)
P2EM$DATE <- aggregtout_P2$DATE

P3EM <-  as.data.frame(EM(aggregtout_P3[,2:4])$X_imp)
P3EM$DATE <- aggregtout_P3$DATE

P4EM <-  as.data.frame(EM(aggregtout_P4[,2:4])$X_imp)
P4EM$DATE <- aggregtout_P4$DATE

P5EM <-  as.data.frame(EM(aggregtout_P5[,2:4])$X_imp)
P5EM$DATE <- aggregtout_P5$DATE
```

```{r}
plot(aggregtout_P1$DATE, aggregtout_P1$O2sat, type="p", pch=16, cex=0.2)
points(P1EM$DATE, P1EM$O2sat , type="p", pch=16, cex=0.2)
points(P1EM$DATE, P1EM$Chltot , type="p", pch=16, cex=0.2,col="blue")

plot(aggregtout_P2$DATE, aggregtout_P2$O2sat, type="p", pch=16, cex=0.2)
points(P2EM$DATE, P2EM$O2sat , type="p", pch=16, cex=0.2)
points(P2EM$DATE, P2EM$Chltot , type="p", pch=16, cex=0.2,col="blue")

plot(aggregtout_P3$DATE, aggregtout_P3$O2sat, type="p", pch=16, cex=0.2)
points(P3EM$DATE, P3EM$O2sat , type="p", pch=16, cex=0.2)
points(P3EM$DATE, P3EM$Chltot , type="p", pch=16, cex=0.2,col="blue")

plot(aggregtout_P4$DATE, aggregtout_P4$O2sat, type="p", pch=16, cex=0.2)
points(P4EM$DATE, P4EM$O2sat , type="p", pch=16, cex=0.2)
points(P4EM$DATE, P4EM$Chltot , type="p", pch=16, cex=0.2,col="blue")

plot(aggregtout_P5$DATE, aggregtout_P5$O2sat, type="p", pch=16, cex=0.2)
points(P5EM$DATE, P5EM$O2sat , type="p", pch=16, cex=0.2)
points(P5EM$DATE, P5EM$Chltot , type="p", pch=16, cex=0.2,col="blue")
```

## Traitement des outliers

####################################################################### A COMPLeter

# Visualisation des données

## Boîte à moustaches

On concatène les données nettoyées pour chaque palier dans un seul dataframe pour faciliter la visualisation. On se penche sur l'idée de faire des boîtes à moustaches pour chaque palier et chaque variable.

```{r}
library(dplyr)

P1EM$palier <- 1
P2EM$palier <- 2
P3EM$palier <- 3
P4EM$palier <- 4
P5EM$palier <- 5

data <- bind_rows(P1EM, P2EM, P3EM, P4EM, P5EM) %>%
  mutate(palier = factor(palier))
```

```{r}
library(ggplot2)


ggplot(data, aes(x = palier, y = Twater, fill = palier)) +
  geom_boxplot() +
  labs(x="Palier", y="Twater") +
  theme_minimal()

ggplot(data, aes(x = palier, y = Twater, fill = palier)) +
  geom_boxplot() +
  labs(x="Palier", y="O2sat") +
  theme_minimal()

ggplot(data, aes(x = palier, y = Twater, fill = palier)) +
  geom_boxplot() +
  labs(x="Palier", y="Chltot") +
  theme_minimal()

```


Il n'y a pas de différences visibles entre les palier pour chaque variable. Bien que les variables on l'air de plus se concentrer autour de la moyenne pour de paliers plus grand.

Ce dont on aurait du s'attendre d'après les premièrs graphiques. En effet, on capte à quelques exceptions près les mêmes tendances pour chaque palier.

## Étude de la volatilité en fonction du palier

En revanche, quelque chose d'intéressant à observer est la volatilité de chaque variable en fonction du palier. En effet, pour la palier 1, il semble que la courbe semble beacoup plus érratique par rapport au palier 5 où la courbe et plus lisse. Nous allons donc étudier la volatilité de chaque variable en fonction du palier.


Notre idée et donc de calculer la variation absolue entre deux instants successifs pour chaque variable et chaque palier. C'est à dire :
$$
\Delta X_t = |X_t - X_{t-1}|
$$

Avec `X ` la variable considérée (`Twater, O2sat ou Chltot`) et `t` l'instant considéré.

```{r}
library(tidyr)

vol <- data %>%
  arrange(palier, DATE) %>%
  group_by(palier) %>%
  mutate(dTwater = abs(Twater - lag(Twater)),
         dO2sat  = abs(O2sat  - lag(O2sat)),
         dChltot = abs(Chltot - lag(Chltot))) %>%
  ungroup() %>%
  pivot_longer(cols = c(dTwater, dO2sat, dChltot),
               names_to = "variable",
               values_to = "abs_delta") %>%
  filter(is.finite(abs_delta)) %>%
  mutate(variable = recode(variable,
                           dTwater="Twater",
                           dO2sat="O2sat",
                           dChltot="Chltot"))

ggplot(vol, aes(x = palier, y = abs_delta, fill = palier)) +
  geom_boxplot(outlier.alpha = 0.2) +
  facet_wrap(~variable, scales = "free_y") +
  theme_minimal() +
  guides(fill = "none") +
  labs(x="Palier", y="|Δ| (variation entre 2 instants)")


```


La volatilité instantanée décroît globalement avec le palier pour `Twater` (série plus lisse aux paliers élevés).

On peut supposer que cela est du au fait que la température de l'eau est plus stable en profondeur qu'à la surface où elle est plus sujette aux variations climatiques (ensoleillement, vent, etc.).

Pour `O2sat` et `Chltot`, la volatilité typique varie peu selon le palier, mais on observe dans tous les cas des épisodes ponctuels de variations extrêmes (surement du à des outliers ou a des phénomènes très particulier qu'on ne pourrait expliquer).









































































<!--
# Traitement des outliers et des données manquantes

Nous allons désormais faire un traitement des outliers sur les données agrégées pour chaque palier. Nous attribueront des NA à ces données manquantes par seuil de densité. Par la suite, nous utiliserons un algorithme EM pour opérer un traitement des valeurs manquantes (comme nous avions fait dans le précédent tp)

## Traitement des outliers

Algorithme EM pour estimer un mélange gaussien
```{r}
EM=function(data, K=3, max_iter=100, tol=1e-6){
  ts=data[!is.na(data)]
  n=length(ts)

  mu_k=as.numeric(quantile(ts,probs=seq(0.25, 0.75, length.out=K)))
  sigma_k=rep(sd(ts), K)
  eps_k=rep(1/K, K)

  loglik_old=-Inf

  for (iter in 1:max_iter){

    # Etape E
    dens=sapply(1:K, function(k)
      eps_k[k]*dnorm(ts, mu_k[k], sigma_k[k])
    )

    dens_sum=rowSums(dens)
    dens_sum[dens_sum<tol]=tol

    gamma=dens/dens_sum

    # Etape M
    N_k=colSums(gamma)

    eps_k=N_k/n
    mu_k=colSums(gamma*matrix(ts,n,K))/N_k
    sigma_k=sqrt(colSums(gamma*sweep(matrix(ts,n,K), 2, mu_k, FUN="-")^2)/N_k)

    sigma_k[sigma_k<tol]=tol

    loglik=sum(log(dens_sum))

    if (abs(loglik - loglik_old)<tol) break
    loglik_old=loglik
  }

  list(eps=eps_k, mu=mu_k, sigma=sigma_k, loglik=loglik)
}
```

Estimation des paramètres pour chaque variable
```{r}
param=function(x){
  list(EM(x$Twater),EM(x$O2sat),EM(x$Chltot))
}
```

Estimation de la densité pour chaque variable
```{r}
dens=function(x,param){
  d=c()
  l=length(x)
  for (i in 1:l){
    s=c()
    for (j in 1:3){
      s=c(s,param$eps[j]*dnorm(x[i],param$mu[j],param$sigma[j]))
    }
    d=c(d,sum(s))
  }
  return(d)
}
```

Attribution des NA pour des densité faibles
```{r}
clean=function(x,d,seuil=0.01){
  l=length(x)
  clean=rep(NA,l)
  for (i in 1:l){
    if (!is.na(d[i]) && d[i] >= seuil){
      clean[i]=x[i]
    }
  }
  return(clean)
}
```


Algorithme EM
```{r}
EM_for_na <- function(X, max_iter=50, tol=1e-4, ridge=1e-6) {
  X <- as.matrix(X)
  n <- nrow(X)
  p <- ncol(X)
  miss <- is.na(X)

  #INITIALISATION
  Ximp <- X
  cm <- colMeans(X, na.rm=TRUE)
  for (j in 1:p){
    Ximp[miss[,j], j] <- cm[j]
  }

  mu <- colMeans(Ximp)
  S <- cov(Ximp) + ridge*diag(p)

  for (it in 1:max_iter) {
    mu0 <- mu; S0 <- S



    #ETAPE E
    for (i in 1:n) {
      mi <- miss[i, ]
      if (!any(mi) || all(mi)) next
      oi <- !mi

      Soo <- S[oi, oi, drop=FALSE] + ridge*diag(sum(oi))
      Sm_o <- S[mi, oi, drop=FALSE]

      x_o <- Ximp[i, oi]
      Ximp[i, mi] <- as.numeric(mu[mi] + Sm_o %*% solve(Soo) %*% (x_o - mu[oi]))
    }


    #ETAPE M
    mu <- colMeans(Ximp)
    S  <- cov(Ximp) + ridge*diag(p)

    dmu <- norm(mu - mu0, "2") / max(norm(mu0, "2"), 1e-12)
    dS  <- norm(S - S0,  "F") / max(norm(S0, "F"), 1e-12)
    crit <- max(dmu, dS)
    if (!is.na(crit) && crit < tol) break
  }

  list(X_imp = Ximp, mu = mu, Sigma = S, n_iter = it)
}
```

## Fonction qui traite les outliers et les données manquantes

On réunit toutes les fonctions précédentes en une seule afin de pouvoir faire toutes nos tâches par palier.

```{r}
aggreg_list=list(aggregtout_P1,aggregtout_P2,aggregtout_P3,aggregtout_P4,aggregtout_P5)

traitement=function(i){
  x=aggreg_list[[i]]
  parametres=param(x)
  d=list(dens(x$Twater,parametres[[1]]),dens(x$O2sat,parametres[[2]]),dens(x$Chltot,parametres[[3]]))
  cleaned_data=list(clean(x=x$Twater,d=d[[1]]),clean(x=x$O2sat,d=d[[2]]),clean(x=x$Chltot,d=d[[3]]))
  #cleaned_data=EM_for_na(as.data.frame(cleaned_data))
  #cleaned_data=data.frame(DATE=x$DATE,cleaned_data)
  return(cleaned_data)
}
```

```{r}
test=traitement(1)
```

```{r}
plot(aggregtout_P1$DATE,test[[1]])
```


 -->
